//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-27506705
// Cuda compilation tools, release 10.2, V10.2.89
// Based on LLVM 3.4svn
//

.version 6.5
.target sm_30
.address_size 64

	// .globl	transformKernel
.const .align 4 .b8 __cudart_i2opi_f[24] = {65, 144, 67, 60, 153, 149, 98, 219, 192, 221, 52, 245, 209, 87, 39, 252, 41, 21, 68, 78, 110, 131, 249, 162};

.visible .entry transformKernel(
	.param .u64 transformKernel_param_0,
	.param .u32 transformKernel_param_1,
	.param .u32 transformKernel_param_2,
	.param .f32 transformKernel_param_3,
	.param .u64 transformKernel_param_4
)
{
	.local .align 4 .b8 	__local_depot0[28];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<23>;
	.reg .f32 	%f<86>;
	.reg .b32 	%r<140>;
	.reg .f64 	%fd<5>;
	.reg .b64 	%rd<40>;


	mov.u64 	%SPL, __local_depot0;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd13, [transformKernel_param_0];
	ld.param.u32 	%r55, [transformKernel_param_1];
	ld.param.u32 	%r56, [transformKernel_param_2];
	ld.param.f32 	%f25, [transformKernel_param_3];
	ld.param.u64 	%rd14, [transformKernel_param_4];
	mul.f32 	%f26, %f25, 0f3F22F983;
	cvt.rni.s32.f32	%r139, %f26;
	cvt.rn.f32.s32	%f27, %r139;
	mov.f32 	%f28, 0fBFC90FDA;
	fma.rn.f32 	%f29, %f27, %f28, %f25;
	mov.f32 	%f30, 0fB3A22168;
	fma.rn.f32 	%f31, %f27, %f30, %f29;
	mov.f32 	%f32, 0fA7C234C5;
	fma.rn.f32 	%f83, %f27, %f32, %f31;
	abs.f32 	%f2, %f25;
	setp.leu.f32	%p1, %f2, 0f47CE4780;
	mov.u32 	%r131, %r139;
	mov.f32 	%f80, %f83;
	@%p1 bra 	BB0_11;

	setp.eq.f32	%p2, %f2, 0f7F800000;
	@%p2 bra 	BB0_10;
	bra.uni 	BB0_2;

BB0_10:
	mov.f32 	%f35, 0f00000000;
	mul.rn.f32 	%f80, %f25, %f35;
	mov.u32 	%r131, %r139;
	bra.uni 	BB0_11;

BB0_2:
	mov.b32 	 %r59, %f25;
	shl.b32 	%r60, %r59, 8;
	or.b32  	%r2, %r60, -2147483648;
	add.u64 	%rd16, %SP, 0;
	add.u64 	%rd37, %SPL, 0;
	mov.u32 	%r125, 0;
	mov.u64 	%rd36, __cudart_i2opi_f;
	mov.u32 	%r124, -6;

BB0_3:
	.pragma "nounroll";
	ld.const.u32 	%r63, [%rd36];
	// inline asm
	{
	mad.lo.cc.u32   %r61, %r63, %r2, %r125;
	madc.hi.u32     %r125, %r63, %r2,  0;
	}
	// inline asm
	st.local.u32 	[%rd37], %r61;
	add.s64 	%rd37, %rd37, 4;
	add.s64 	%rd36, %rd36, 4;
	add.s32 	%r124, %r124, 1;
	setp.ne.s32	%p3, %r124, 0;
	@%p3 bra 	BB0_3;

	bfe.u32 	%r67, %r59, 23, 8;
	add.s32 	%r68, %r67, -128;
	shr.u32 	%r69, %r68, 5;
	and.b32  	%r7, %r59, -2147483648;
	cvta.to.local.u64 	%rd18, %rd16;
	st.local.u32 	[%rd18+24], %r125;
	bfe.u32 	%r8, %r59, 23, 5;
	mov.u32 	%r70, 6;
	sub.s32 	%r71, %r70, %r69;
	mul.wide.s32 	%rd19, %r71, 4;
	add.s64 	%rd6, %rd18, %rd19;
	ld.local.u32 	%r127, [%rd6];
	ld.local.u32 	%r126, [%rd6+-4];
	setp.eq.s32	%p4, %r8, 0;
	@%p4 bra 	BB0_6;

	mov.u32 	%r72, 32;
	sub.s32 	%r73, %r72, %r8;
	shr.u32 	%r74, %r126, %r73;
	shl.b32 	%r75, %r127, %r8;
	add.s32 	%r127, %r74, %r75;
	ld.local.u32 	%r76, [%rd6+-8];
	shr.u32 	%r77, %r76, %r73;
	shl.b32 	%r78, %r126, %r8;
	add.s32 	%r126, %r77, %r78;

BB0_6:
	shr.u32 	%r79, %r126, 30;
	shl.b32 	%r80, %r127, 2;
	add.s32 	%r129, %r80, %r79;
	shl.b32 	%r16, %r126, 2;
	shr.u32 	%r81, %r129, 31;
	shr.u32 	%r82, %r127, 30;
	add.s32 	%r17, %r81, %r82;
	setp.eq.s32	%p5, %r81, 0;
	@%p5 bra 	BB0_7;

	not.b32 	%r83, %r129;
	neg.s32 	%r128, %r16;
	setp.eq.s32	%p6, %r16, 0;
	selp.u32	%r84, 1, 0, %p6;
	add.s32 	%r129, %r84, %r83;
	xor.b32  	%r130, %r7, -2147483648;
	bra.uni 	BB0_9;

BB0_7:
	mov.u32 	%r128, %r16;
	mov.u32 	%r130, %r7;

BB0_9:
	cvt.u64.u32	%rd20, %r129;
	shl.b64 	%rd21, %rd20, 32;
	cvt.u64.u32	%rd22, %r128;
	or.b64  	%rd23, %rd21, %rd22;
	cvt.rn.f64.s64	%fd1, %rd23;
	mul.f64 	%fd2, %fd1, 0d3BF921FB54442D19;
	cvt.rn.f32.f64	%f33, %fd2;
	neg.f32 	%f34, %f33;
	setp.eq.s32	%p7, %r130, 0;
	selp.f32	%f80, %f33, %f34, %p7;
	setp.eq.s32	%p8, %r7, 0;
	neg.s32 	%r85, %r17;
	selp.b32	%r131, %r17, %r85, %p8;

BB0_11:
	add.s32 	%r26, %r131, 1;
	and.b32  	%r27, %r26, 1;
	setp.eq.s32	%p9, %r27, 0;
	selp.f32	%f6, %f80, 0f3F800000, %p9;
	mul.rn.f32 	%f7, %f80, %f80;
	mov.f32 	%f37, 0f00000000;
	fma.rn.f32 	%f8, %f7, %f6, %f37;
	mov.f32 	%f81, 0fB94D4153;
	@%p9 bra 	BB0_13;

	mov.f32 	%f38, 0fBAB607ED;
	mov.f32 	%f39, 0f37CBAC00;
	fma.rn.f32 	%f81, %f39, %f7, %f38;

BB0_13:
	selp.f32	%f40, 0f3C0885E4, 0f3D2AAABB, %p9;
	fma.rn.f32 	%f41, %f81, %f7, %f40;
	selp.f32	%f42, 0fBE2AAAA8, 0fBEFFFFFF, %p9;
	fma.rn.f32 	%f43, %f41, %f7, %f42;
	fma.rn.f32 	%f82, %f43, %f8, %f6;
	and.b32  	%r86, %r26, 2;
	setp.eq.s32	%p11, %r86, 0;
	@%p11 bra 	BB0_15;

	mov.f32 	%f45, 0fBF800000;
	fma.rn.f32 	%f82, %f82, %f45, %f37;

BB0_15:
	@%p1 bra 	BB0_26;

	setp.eq.f32	%p13, %f2, 0f7F800000;
	@%p13 bra 	BB0_25;
	bra.uni 	BB0_17;

BB0_25:
	mul.rn.f32 	%f83, %f25, %f37;
	bra.uni 	BB0_26;

BB0_17:
	mov.b32 	 %r28, %f25;
	shr.u32 	%r29, %r28, 23;
	shl.b32 	%r89, %r28, 8;
	or.b32  	%r30, %r89, -2147483648;
	add.u64 	%rd25, %SP, 0;
	add.u64 	%rd39, %SPL, 0;
	mov.u32 	%r133, 0;
	mov.u64 	%rd38, __cudart_i2opi_f;
	mov.u32 	%r132, -6;

BB0_18:
	.pragma "nounroll";
	ld.const.u32 	%r92, [%rd38];
	// inline asm
	{
	mad.lo.cc.u32   %r90, %r92, %r30, %r133;
	madc.hi.u32     %r133, %r92, %r30,  0;
	}
	// inline asm
	st.local.u32 	[%rd39], %r90;
	add.s64 	%rd39, %rd39, 4;
	add.s64 	%rd38, %rd38, 4;
	add.s32 	%r132, %r132, 1;
	setp.ne.s32	%p14, %r132, 0;
	@%p14 bra 	BB0_18;

	and.b32  	%r95, %r29, 255;
	add.s32 	%r96, %r95, -128;
	shr.u32 	%r97, %r96, 5;
	and.b32  	%r35, %r28, -2147483648;
	cvta.to.local.u64 	%rd27, %rd25;
	st.local.u32 	[%rd27+24], %r133;
	mov.u32 	%r98, 6;
	sub.s32 	%r99, %r98, %r97;
	mul.wide.s32 	%rd28, %r99, 4;
	add.s64 	%rd12, %rd27, %rd28;
	ld.local.u32 	%r135, [%rd12];
	ld.local.u32 	%r134, [%rd12+-4];
	and.b32  	%r38, %r29, 31;
	setp.eq.s32	%p15, %r38, 0;
	@%p15 bra 	BB0_21;

	mov.u32 	%r100, 32;
	sub.s32 	%r101, %r100, %r38;
	shr.u32 	%r102, %r134, %r101;
	shl.b32 	%r103, %r135, %r38;
	add.s32 	%r135, %r102, %r103;
	ld.local.u32 	%r104, [%rd12+-8];
	shr.u32 	%r105, %r104, %r101;
	shl.b32 	%r106, %r134, %r38;
	add.s32 	%r134, %r105, %r106;

BB0_21:
	shr.u32 	%r107, %r134, 30;
	shl.b32 	%r108, %r135, 2;
	add.s32 	%r137, %r108, %r107;
	shl.b32 	%r44, %r134, 2;
	shr.u32 	%r109, %r137, 31;
	shr.u32 	%r110, %r135, 30;
	add.s32 	%r45, %r109, %r110;
	setp.eq.s32	%p16, %r109, 0;
	@%p16 bra 	BB0_22;

	not.b32 	%r111, %r137;
	neg.s32 	%r136, %r44;
	setp.eq.s32	%p17, %r44, 0;
	selp.u32	%r112, 1, 0, %p17;
	add.s32 	%r137, %r112, %r111;
	xor.b32  	%r138, %r35, -2147483648;
	bra.uni 	BB0_24;

BB0_22:
	mov.u32 	%r136, %r44;
	mov.u32 	%r138, %r35;

BB0_24:
	cvt.u64.u32	%rd29, %r137;
	shl.b64 	%rd30, %rd29, 32;
	cvt.u64.u32	%rd31, %r136;
	or.b64  	%rd32, %rd30, %rd31;
	cvt.rn.f64.s64	%fd3, %rd32;
	mul.f64 	%fd4, %fd3, 0d3BF921FB54442D19;
	cvt.rn.f32.f64	%f46, %fd4;
	neg.f32 	%f47, %f46;
	setp.eq.s32	%p18, %r138, 0;
	selp.f32	%f83, %f46, %f47, %p18;
	setp.eq.s32	%p19, %r35, 0;
	neg.s32 	%r113, %r45;
	selp.b32	%r139, %r45, %r113, %p19;

BB0_26:
	and.b32  	%r54, %r139, 1;
	setp.eq.s32	%p20, %r54, 0;
	selp.f32	%f17, %f83, 0f3F800000, %p20;
	mul.rn.f32 	%f18, %f83, %f83;
	fma.rn.f32 	%f19, %f18, %f17, %f37;
	mov.f32 	%f84, 0fB94D4153;
	@%p20 bra 	BB0_28;

	mov.f32 	%f51, 0fBAB607ED;
	mov.f32 	%f52, 0f37CBAC00;
	fma.rn.f32 	%f84, %f52, %f18, %f51;

BB0_28:
	selp.f32	%f53, 0f3C0885E4, 0f3D2AAABB, %p20;
	fma.rn.f32 	%f54, %f84, %f18, %f53;
	selp.f32	%f55, 0fBE2AAAA8, 0fBEFFFFFF, %p20;
	fma.rn.f32 	%f56, %f54, %f18, %f55;
	fma.rn.f32 	%f85, %f56, %f19, %f17;
	and.b32  	%r114, %r139, 2;
	setp.eq.s32	%p22, %r114, 0;
	@%p22 bra 	BB0_30;

	mov.f32 	%f58, 0fBF800000;
	fma.rn.f32 	%f85, %f85, %f58, %f37;

BB0_30:
	cvt.rn.f32.s32	%f59, %r55;
	mov.u32 	%r115, %ntid.x;
	mov.u32 	%r116, %ctaid.x;
	mov.u32 	%r117, %tid.x;
	mad.lo.s32 	%r118, %r115, %r116, %r117;
	cvt.rn.f32.u32	%f60, %r118;
	mul.f32 	%f61, %f59, 0f3F000000;
	sub.f32 	%f62, %f60, %f61;
	cvt.rn.f32.s32	%f63, %r56;
	mov.u32 	%r119, %ntid.y;
	mov.u32 	%r120, %ctaid.y;
	mov.u32 	%r121, %tid.y;
	mad.lo.s32 	%r122, %r119, %r120, %r121;
	cvt.rn.f32.u32	%f64, %r122;
	mul.f32 	%f65, %f63, 0f3F000000;
	sub.f32 	%f66, %f64, %f65;
	mul.f32 	%f67, %f66, %f85;
	mul.f32 	%f68, %f62, %f82;
	sub.f32 	%f69, %f68, %f67;
	mul.f32 	%f70, %f62, %f85;
	fma.rn.f32 	%f71, %f66, %f82, %f70;
	div.rn.f32 	%f72, %f69, %f59;
	div.rn.f32 	%f73, %f71, %f63;
	add.f32 	%f74, %f72, 0f3F000000;
	add.f32 	%f75, %f73, 0f3F000000;
	tex.2d.v4.f32.f32	{%f76, %f77, %f78, %f79}, [%rd14, {%f74, %f75}];
	mad.lo.s32 	%r123, %r122, %r55, %r118;
	cvta.to.global.u64 	%rd33, %rd13;
	mul.wide.u32 	%rd34, %r123, 4;
	add.s64 	%rd35, %rd33, %rd34;
	st.global.f32 	[%rd35], %f76;
	ret;
}


